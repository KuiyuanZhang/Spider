{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "https://www.jianshu.com/p/749c7866f211\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class LagouzpItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    city = scrapy.Field()\n",
    "    company_name = scrapy.Field()\n",
    "    size = scrapy.Field()\n",
    "    edu = scrapy.Field()\n",
    "    financeStage = scrapy.Field()\n",
    "    firstType = scrapy.Field()\n",
    "    industryField = scrapy.Field()\n",
    "    name = scrapy.Field()\n",
    "    salary = scrapy.Field()\n",
    "    secondType = scrapy.Field()\n",
    "    workYear = scrapy.Field()\n",
    "    time = scrapy.Field()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'MySQLdb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-47f93c711da0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mMySQLdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLagouzpPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#写文件\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'MySQLdb'"
     ]
    }
   ],
   "source": [
    "import MySQLdb\n",
    "import csv\n",
    "\n",
    "class LagouzpPipeline(object):\n",
    "    #写文件\n",
    "    # def __init__(self):\n",
    "    #     with open(\"data.csv\", \"ab+\") as self.files:\n",
    "    #         self.write = csv.writer(self.files)\n",
    "    #         self.write.writerow(\n",
    "    #             ['职位名称', '公司名称', '城市', '公司规模', '公司类型', '月薪', '行业领域', 'firstType', 'senondType', '工作经历', '学历', '发布时间'])\n",
    "    #\n",
    "    # def process_item(self, item, spider):\n",
    "    #     with open(\"data.csv\", \"ab+\") as self.files:\n",
    "    #         self.write = csv.writer(self.files)\n",
    "    #         self.line = [item['name'], item['city'], item['company_name'], item['size'], item['financeStage'],item['salary'], item['industryField'], item['firstType'],item['secondType'],item['workYear'],item['edu'],item['time']]\n",
    "    #         self.write.writerow(self.line)\n",
    "    #     return item\n",
    "    #数据库\n",
    "    def __init__(self):\n",
    "        self.conn = MySQLdb.connect(user='root', passwd='123456', db='lagou', host='localhost', charset='utf8',\n",
    "                                    use_unicode=True)\n",
    "        self.cursor = self.conn.cursor()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.cursor.execute(\n",
    "            \"insert into jobinfo(name,city,company,size,type,salary,field,firsttype,secondtype,workyear,edu,time) values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\",(item['name'], item['city'], item['company_name'], item['size'], item['financeStage'],item['salary'], item['industryField'], item['firstType'],item['secondType'],item['workYear'],item['edu'],item['time'],))\n",
    "        self.conn.commit()\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "class Spider(scrapy.Spider):\n",
    "    name = 'lagou'\n",
    "    cookies = {\n",
    "        'user_trace_token': '20170314211704-f55f18938db84cfeae95d1efec6d585e',\n",
    "        'LGUID': '20170314211706-859943f0-08b8-11e7-93e0-5254005c3644',\n",
    "        'JSESSIONID': 'AA1DE67564F4C20F86F89F3572B706A1',\n",
    "        'PRE_UTM': '',\n",
    "        'PRE_HOST': 'www.baidu.com',\n",
    "        'PRE_SITE': 'https%3A%2F%2Fwww.baidu.com%2Flink%3Furl%3DuQkzN6ld65B8UHLJeaN2RVwWb3jiAl6AkSQSZRkXpRC%26wd%3D%26eqid%3Df6aa96cc0000dd5e0000000258ff3f34',\n",
    "        'PRE_LAND': 'https%3A%2F%2Fwww.lagou.com%2F',\n",
    "        'index_location_city': '%E5%85%A8%E5%9B%BD',\n",
    "        'Hm_lvt_4233e74dff0ae5bd0a3d81c6ccf756e6': '1491116405,1491116452,1493122880,1493122898',\n",
    "        'Hm_lpvt_4233e74dff0ae5bd0a3d81c6ccf756e6': '1493123186',\n",
    "        '_ga': 'GA1.2.1412866745.1489497427',\n",
    "        'LGSID': '20170425202132-b7ea71dc-29b1-11e7-bc70-525400f775ce',\n",
    "        'LGRID': '20170425202620-6394f6bd-29b2-11e7-bc72-525400f775ce',\n",
    "        'TG-TRACK-CODE': 'search_code',\n",
    "        'SEARCH_ID': '63e7755cfbbf40559a5dac6a35e5f49f'\n",
    "    }\n",
    "    headers = {\n",
    "        \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\"}\n",
    "    def start_requests(self):\n",
    "        kd = ['python工程师', 'python数据分析']\n",
    "        city = ['北京', '上海', '深圳', '广州', '杭州', '成都', '南京', '武汉', '西安', '厦门', '长沙', '苏州', '天津']\n",
    "        urls_kd = ['https://www.lagou.com/jobs/list_{}?px=default&city='.format(one) for one in kd]\n",
    "        for urls in urls_kd:\n",
    "            urls_city = [urls + one for one in city]\n",
    "            for url in urls_city:\n",
    "                response = requests.get(url, headers=self.headers, cookies=self.cookies)\n",
    "                location = url.split('&')[-1].split('=')[1]\n",
    "                key = url.split('/')[-1].split('?')[0].split('_')[1]\n",
    "                soup = BeautifulSoup(response.text, 'lxml')\n",
    "                pages = soup.find('span', {'class': 'span totalNum'}).get_text()\n",
    "                for i in range(1, int(pages) + 1):\n",
    "                    url = 'https://www.lagou.com/jobs/positionAjax.json?px=default&city={}&needAddtionalResult=false'.format(location)\n",
    "                    formdata = {\n",
    "                        'first': 'true',\n",
    "                        'pn': str(i),\n",
    "                        'kd': key\n",
    "                    }\n",
    "                    print ('正在获取职位——{}，城市{},第{}页数据'.format(key,location,i))\n",
    "                    yield scrapy.FormRequest(url,formdata=formdata,cookies=self.cookies,callback=self.parse)\n",
    "    def parse(self, response):\n",
    "        data = json.loads(response.text)\n",
    "        content = data['content']\n",
    "        positionResult = content['positionResult']\n",
    "        item = LagouzpItem()\n",
    "        for one in positionResult['result']:\n",
    "            try:\n",
    "                item['city'] = one['city']\n",
    "            except:\n",
    "                item['city'] = u''\n",
    "            try:\n",
    "                item['company_name'] = one['companyFullName']\n",
    "            except:\n",
    "                item['company_name'] = u''\n",
    "            try:\n",
    "                item['size'] = one['companySize']\n",
    "            except:\n",
    "                item['size'] = u''\n",
    "            try:\n",
    "                item['edu'] = one['education']\n",
    "            except:\n",
    "                item['edu'] = u''\n",
    "            try:\n",
    "                item['financeStage'] = one['financeStage']\n",
    "            except:\n",
    "                item['financeStage'] = u''\n",
    "            try:\n",
    "                item['firstType'] = one['firstType']\n",
    "            except:\n",
    "                item['firstType'] = u''\n",
    "            try:\n",
    "                item['industryField'] = one['industryField']\n",
    "            except:\n",
    "                item['industryField'] = u''\n",
    "            try:\n",
    "                item['name']= one['positionName']\n",
    "            except:\n",
    "                item['name'] = u''\n",
    "            try:\n",
    "                item['salary'] = one['salary']\n",
    "            except:\n",
    "                item['salary'] = u''\n",
    "            try:\n",
    "                item['secondType'] = one['secondType']\n",
    "            except:\n",
    "                item['secondType'] = u''\n",
    "            try:\n",
    "                item['workYear'] = one['workYear']\n",
    "            except:\n",
    "                item['workYear'] = u''\n",
    "            try:\n",
    "                item['time'] = one['createTime'].split(' ')[0]\n",
    "            except:\n",
    "                item['time'] = u''\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
